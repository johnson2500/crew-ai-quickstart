import os
import glob
import uuid
import json
import requests
import traceback
import logging
from typing import List, Optional, Dict, Any
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from llama_stack_client import LlamaStackClient
from lib.models import ChatRequest, ChatResponse, IngestResponse
from lib.logger import logger
from lib.prompts import RAG_SYSTEM_PROMPT, RAG_NO_CONTEXT_PROMPT, NORMAL_SYSTEM_PROMPT
from config import LLAMA_STACK_URL, KNOWLEDGE_FOLDER, VECTOR_DB_ID, SESSION_STORAGE

# Ensure knowledge folder exists
os.makedirs(KNOWLEDGE_FOLDER, exist_ok=True)

app = FastAPI(title="Llama Stack RAG Backend")

try:
    client = LlamaStackClient(base_url=LLAMA_STACK_URL)
    logger.info(f"Connected to Llama Stack at {LLAMA_STACK_URL}")
    # Print out the client attributes to see what is actually available
    # I fing using rest api for now because the sdk is not working
    logger.info(f"DEBUG: Client Attributes: {[x for x in dir(client) if not x.startswith('_')]}")
except Exception as e:
    logger.info(f"SDK Connection Error: {e}")
    client = None

def get_client():
    client = LlamaStackClient(base_url=LLAMA_STACK_URL)
    logger.info(f"Connected to Llama Stack at {LLAMA_STACK_URL}")
    # Print out the client attributes to see what is actually available
    # I fing using rest api for now because the sdk is not working
    logger.info(f"DEBUG: Client Attributes: {[x for x in dir(client) if not x.startswith('_')]}")
    return client

def raw_chat_completion(model_id: str, messages: List[Dict]):
    """Direct HTTP call to Llama Stack Inference API."""
    url = f"{LLAMA_STACK_URL}/v1/chat/completions"
    payload = {"model": model_id, "messages": messages, "stream": False}
    logger.info(f"DEBUG: Sending Raw Request to {url} with model {model_id}")
    
    resp = requests.post(f"{LLAMA_STACK_URL}/v1/chat/completions", json=payload)
        
    resp.raise_for_status()
    return resp.json()

def raw_vector_query(query: str):
    """Direct HTTP call to Llama Stack Vector IO Query."""
    url = f"{LLAMA_STACK_URL}/v1/vector_io/query"
    payload = {
        "vector_db_id": VECTOR_DB_ID,
        "query": query,
        "params": {"top_k": 3}
    }
    try:
        resp = requests.post(url, json=payload)

        logger.info(f"Raw Vector Query Response: {resp.json()}")
        if resp.status_code == 200:
            return resp.json()
    except Exception as e:
        logger.info(f"Raw Vector Query Error: {e}")
    return None

def raw_ingest_documents(documents: List[Dict]):
    """Direct HTTP call to insert documents."""
    # Try standard vector_io insert
    url = f"{LLAMA_STACK_URL}/v1/tool-runtime/rag-tool/insert"
    payload = {
        "chunk_size_in_tokens": 512,
        "documents": documents,
        "vector_store_id": VECTOR_DB_ID
    }

    logger.info(f"Raw Ingest Documents Payload: {payload}")
    
    resp = requests.post(url, json=payload)

    if resp.status_code == 200:
        logger.info(f"Raw Ingest Documents Response: {resp.json()}")
        return resp.json()
    else:
        logger.info(f"Raw Ingest Documents Error: {resp.status_code} {resp.text}")
        return None


# --- Helper Functions: Logic ---

def get_or_create_vector_db():
    """Creates a vector database if it doesn't exist, following the pattern from upload.py."""
    # First, try to check if vector database exists via SDK
    vector_db_exists = False
    
    # Try SDK approach first (if vector_dbs is available)
    if client:
        try:
            # Check if vector_dbs attribute exists (may not be in all client versions)
            if hasattr(client, 'vector_dbs'):
                stores = client.vector_dbs.list()
                for store in stores:
                    if hasattr(store, 'identifier') and store.identifier == VECTOR_DB_ID:
                        vector_db_exists = True
                        logger.info(f"Vector DB '{VECTOR_DB_ID}' already exists")
                        return VECTOR_DB_ID
                    elif hasattr(store, 'id') and store.id == VECTOR_DB_ID:
                        vector_db_exists = True
                        logger.info(f"Vector DB '{VECTOR_DB_ID}' already exists")
                        return VECTOR_DB_ID
        except Exception as e:
            logger.info(f"Could not check vector_dbs via SDK: {e}")
        
        # Try vector_stores as alternative
        if not vector_db_exists and hasattr(client, 'vector_stores'):
            try:
                stores = client.vector_stores.list()
                for store in stores:
                    if hasattr(store, 'id') and store.id == VECTOR_DB_ID:
                        vector_db_exists = True
                        logger.info(f"Vector Store '{VECTOR_DB_ID}' already exists")
                        return VECTOR_DB_ID
            except Exception as e:
                logger.info(f"Could not check vector_stores: {e}")
    
    # If not found, create it
    if not vector_db_exists:
        logger.info(f"Vector DB '{VECTOR_DB_ID}' not found, creating new one...")
        
        # Get the vector_io provider_id
        provider_id = None
        if client and hasattr(client, 'providers'):
            try:
                providers = client.providers.list()
                for provider in providers:
                    if hasattr(provider, 'api') and provider.api == "vector_io":
                        provider_id = provider.provider_id
                        break
            except Exception as e:
                logger.info(f"Could not get providers: {e}")
        
        # Default provider if not found
        if not provider_id:
            provider_id = "faiss"  # Default to faiss for local storage
            logger.info(f"Using default provider: {provider_id}")
        
        # Try to register via SDK
        if client and hasattr(client, 'vector_dbs'):
            try:
                vector_db = client.vector_dbs.register(
                    vector_db_id=VECTOR_DB_ID,
                    embedding_dimension=384,
                    embedding_model="all-MiniLM-L6-v2",
                    provider_id=provider_id
                )
                logger.info(f"Successfully created Vector DB '{VECTOR_DB_ID}' via SDK")
                return VECTOR_DB_ID
            except Exception as e:
                logger.info(f"SDK vector_dbs.register failed: {e}")
        
        # Fallback to REST API
        try:
            url = f"{LLAMA_STACK_URL}/v1/vector_dbs/register"
            payload = {
                "vector_db_id": VECTOR_DB_ID,
                "embedding_dimension": 384,
                "embedding_model": "all-MiniLM-L6-v2",
                "provider_id": provider_id
            }
            resp = requests.post(url, json=payload)
            if resp.status_code == 200:
                logger.info(f"Successfully created Vector DB '{VECTOR_DB_ID}' via REST API")
                return VECTOR_DB_ID
            elif resp.status_code == 409:  # Already exists
                logger.info(f"Vector DB '{VECTOR_DB_ID}' already exists (409)")
                return VECTOR_DB_ID
            else:
                logger.info(f"Failed to create vector DB via REST: {resp.status_code} {resp.text}")
        except Exception as e:
            logger.info(f"REST API vector_dbs.register failed: {e}")
            logger.info(f"Note: Vector DB may be created automatically on first insert")
    
    return VECTOR_DB_ID

def perform_manual_rag(query: str) -> str:
    """
    return the context string for the query
    """
    logger.info(f"üîç RAG PIPELINE TRIGGERED - Query: '{query[:100]}...' (truncated)" if len(query) > 100 else f"üîç RAG PIPELINE TRIGGERED - Query: '{query}'")
    logger.info(f"üìä RAG Configuration - Vector DB ID: {VECTOR_DB_ID} - Client: {client}")
    context_parts = []
    
    # 1. Try SDK rag_tool.query (Preferred method - matches tool_runtime.py API)
    if client and hasattr(client, 'tool_runtime') and hasattr(client.tool_runtime, 'rag_tool'):
        logger.info("üîÑ Attempting RAG query via rag_tool.query (REST API)")
        logger.info(f"üîë Using Vector Store ID: {VECTOR_DB_ID}")
        try:
            results = requests.post(
                f"{LLAMA_STACK_URL}/v1/tool-runtime/rag-tool/query",
                json={"content": query, "vector_store_ids": [VECTOR_DB_ID]},
                timeout=30
            )
            data = results.json()
            logger.info(f"Raw RAG Query Response: {data}")
            
            # Parse the response - rag_tool.query may return content directly or in chunks
            if not isinstance(data, dict):
                logger.warning("RAG query returned non-dict response")
            elif "chunks" in data and data["chunks"]:
                # Extract content from chunks
                items = data["chunks"] if isinstance(data["chunks"], list) else [data["chunks"]]
                for item in items:
                    if isinstance(item, dict):
                        text = item.get("content") or item.get("text", "")
                    else:
                        text = str(item)
                    if text:
                        context_parts.append(text)
            elif "content" in data:
                # Extract direct content field
                if data["content"]:
                    context_parts.append(data["content"])
                else:
                    logger.warning("RAG query returned empty content")
            else:
                # Fallback: try other common response keys
                for key in ["text", "message", "result"]:
                    if key in data and data[key]:
                        context_parts.append(str(data[key]))
                        break
        except Exception as e:
            logger.error(f"RAG query failed: {e}")
            import traceback
            traceback.logger.info_exc()
    
    if not context_parts:
        logger.warning("‚ö†Ô∏è  RAG PIPELINE COMPLETED - No context retrieved from knowledge base")
        return ""

    # Filter and convert context_parts to strings, removing None/empty values
    context_parts_clean = []
    for part in context_parts:
        if part is not None:
            part_str = str(part).strip()
            if part_str:
                context_parts_clean.append(part_str)
    
    if not context_parts_clean:
        logger.warning("‚ö†Ô∏è  RAG PIPELINE COMPLETED - No valid context retrieved from knowledge base")
        return ""

    total_context_length = sum(len(cp) for cp in context_parts_clean)
    logger.info(f"‚úÖ RAG PIPELINE COMPLETED - Retrieved {len(context_parts_clean)} chunks, {total_context_length} total characters")
    context_str = "\n\n---\nRelevant Context from Knowledge Base:\n" + "\n".join(context_parts_clean) + "\n---\n"
    logger.debug(f"üìù Final context string length: {len(context_str)} characters")
    return context_str

# --- Endpoints ---

@app.get("/health")
def health_check():
    """Health check endpoint to verify API and Llama Stack connection."""
    status = {
        "api": "healthy",
        "llama_stack_url": LLAMA_STACK_URL,
        "client_connected": client is not None,
        "vector_db_id": VECTOR_DB_ID,
        "knowledge_folder": KNOWLEDGE_FOLDER,
        "knowledge_folder_exists": os.path.exists(KNOWLEDGE_FOLDER)
    }
    
    # Check if knowledge folder has files
    if os.path.exists(KNOWLEDGE_FOLDER):
        files = glob.glob(os.path.join(KNOWLEDGE_FOLDER, "*.txt")) + \
                glob.glob(os.path.join(KNOWLEDGE_FOLDER, "*.md"))
        status["knowledge_files_count"] = len(files)
    
    return status

@app.get("/models")
def list_models():
    """Lists models but FILTERS OUT embedding models to prevent 400 errors."""
    models = []
    
    # 1. Try SDK
    if client and hasattr(client, 'models'):
        try:
            models = [m.identifier for m in client.models.list()]
        except: pass
    
    # 2. Try REST
    if not models:
        try:
            resp = requests.get(f"{LLAMA_STACK_URL}/models/list")
            if resp.status_code == 200:
                data = resp.json()
                # Handle different list shapes
                if isinstance(data, list): models = [m.get("identifier") for m in data]
                elif "data" in data: models = [m.get("identifier") for m in data["data"]]
        except: pass

    # 3. Fallback
    if not models:
        return ["meta-llama/Llama-3.1-8B-Instruct", "meta-llama/Llama-3.2-3B-Instruct"]

    # --- CRITICAL FIX: Filter out non-chat models ---
    # 'all-minilm' caused your error because it cannot chat.
    chat_models = [
        m for m in models 
        if "minilm" not in m.lower() 
        and "bert" not in m.lower() 
        and "embedding" not in m.lower()
    ]
    
    # If filter killed everything (unlikely), return original list
    return chat_models if chat_models else models

@app.post("/chat", response_model=ChatResponse)
def chat_endpoint(request: ChatRequest):
    # 1. Manage Session
    session_id = request.session_id or str(uuid.uuid4())

    # 2. Prepare RAG Context (do this first to determine system prompt)
    context_str = ""
    if request.use_rag:
        logger.info(f"üöÄ RAG REQUESTED for session {session_id} - Model: {request.model}")
        logger.info(f"üìù User message: '{request.message[:100]}...' (truncated)" if len(request.message) > 100 else f"üìù User message: '{request.message}'")
        context_str = perform_manual_rag(request.message)
        if context_str:
            logger.info(f"‚úÖ RAG context retrieved and will be added to prompt")
        else:
            logger.warning(f"‚ö†Ô∏è  RAG was requested but no context was retrieved - proceeding without RAG context")
    else:
        logger.debug(f"‚ÑπÔ∏è  RAG not requested (use_rag=False) for session {session_id}")
    
    # Determine system prompt based on RAG request and context availability
    if request.use_rag:
        system_prompt = RAG_SYSTEM_PROMPT if context_str else RAG_NO_CONTEXT_PROMPT
        if context_str:
            logger.info("üîí RAG-only mode enabled: Model restricted to knowledge base context only")
        else:
            logger.info("üîí RAG mode enabled but no context found")
    else:
        system_prompt = NORMAL_SYSTEM_PROMPT
    
    # Initialize or update session with appropriate system prompt
    if session_id not in SESSION_STORAGE:
        SESSION_STORAGE[session_id] = [{"role": "system", "content": system_prompt}]
    else:
        # Update existing session's system prompt
        if SESSION_STORAGE[session_id] and SESSION_STORAGE[session_id][0].get("role") == "system":
            SESSION_STORAGE[session_id][0]["content"] = system_prompt
            logger.info("üîí Updated existing session system prompt")
        else:
            SESSION_STORAGE[session_id].insert(0, {"role": "system", "content": system_prompt})
            logger.info("üîí Added system prompt to existing session")
    
    # Get history - use reference directly since we update SESSION_STORAGE in place above
    history = SESSION_STORAGE[session_id]
    
    # 3. Construct Prompt
    final_user_content = request.message
    if context_str and context_str.strip():
        # When RAG is used, format the context prominently with strict instructions
        # Extract just the context content (remove the wrapper)
        if 'Relevant Context from Knowledge Base:' in context_str:
            try:
                context_content = context_str.split('Relevant Context from Knowledge Base:')[1].split('---')[0].strip()
            except (IndexError, AttributeError):
                logger.warning("‚ö†Ô∏è  Failed to parse context string, using full context_str")
                context_content = context_str.strip()
        else:
            context_content = context_str.strip()
        
        if context_content:
            final_user_content = f"""Relevant Context from Knowledge Base:
                {context_content}
                IMPORTANT: Answer the following question using ONLY the information from the 
                context above. Do not use any other knowledge. If the answer is not in the context, 
                clearly state that you don't have that information in your knowledge base.
                Question: {request.message}
            """
            logger.info(f"üìã Final prompt constructed with RAG context ({len(final_user_content)} characters total, context: {len(context_content)} chars)")
        else:
            logger.warning("‚ö†Ô∏è  RAG context was retrieved but is empty after parsing - proceeding without context")
            logger.debug(f"üìã Final prompt constructed without RAG context ({len(final_user_content)} characters)")
    else:
        logger.debug(f"üìã Final prompt constructed without RAG context ({len(final_user_content)} characters)")

    # Prepare payload - ensure system prompt is included
    messages_payload = history + [{"role": "user", "content": final_user_content}]
    
    # Verify system prompt is present (should be first message)
    if messages_payload and messages_payload[0].get("role") != "system":
        logger.warning("‚ö†Ô∏è  System prompt missing from messages_payload, adding default")
        messages_payload.insert(0, {"role": "system", "content": "You are a helpful assistant."})
    
    # Log the payload structure for debugging
    logger.debug(f"üì§ Messages payload: {len(messages_payload)} messages (system: {messages_payload[0].get('role') == 'system'}, user: {len([m for m in messages_payload if m.get('role') == 'user'])} messages)")

    try:
        bot_content = ""
        
        # Strategy: Try SDK Inference -> Fail to REST Inference
        if client and hasattr(client, 'inference'):
            try:
                response = client.inference.chat_completion(
                    model_id=request.model,
                    messages=messages_payload,
                    stream=False
                )
                if hasattr(response, 'completion_message'):
                    bot_content = response.completion_message.content
                elif hasattr(response, 'choices'):
                    bot_content = response.choices[0].message.content
                else:
                    bot_content = str(response)
            except Exception as sdk_e:
                logger.error(f"SDK Inference failed ({sdk_e}), switching to REST...")
                raise Exception("Trigger REST Fallback") # Force catch block
        else:
            raise Exception("Inference API missing in SDK")

    except Exception:
        # Fallback to Raw REST
        try:
            data = raw_chat_completion(request.model, messages_payload)
            
            # Extract content from REST JSON
            if "completion_message" in data:
                bot_content = data["completion_message"].get("content", "")
            elif "choices" in data and len(data["choices"]) > 0:
                bot_content = data["choices"][0].get("message", {}).get("content", "")
            else:
                bot_content = str(data)
                
        except Exception as rest_e:
            logger.error(f"REST Inference failed ({rest_e}), switching to REST...") 
            raise HTTPException(status_code=500, detail=f"Llama Stack Error: {str(rest_e)}")

    # Clean up list content (multimodal)
    if isinstance(bot_content, list):
         bot_content = " ".join([
             getattr(b, 'text', '') or (b.get('text') if isinstance(b, dict) else str(b)) 
             for b in bot_content
         ])
    
    bot_content = str(bot_content)

    # 4. Update History
    history.append({"role": "user", "content": request.message})
    history.append({"role": "assistant", "content": bot_content})
    logger.info(f"History: {history}")
    logger.info(f"Session ID: {session_id}")
    logger.info(f"Bot Content: {bot_content}")
    SESSION_STORAGE[session_id] = history

    return ChatResponse(response=bot_content, session_id=session_id)

@app.post("/ingest", response_model=IngestResponse)
def ingest_knowledge():
    files = glob.glob(os.path.join(KNOWLEDGE_FOLDER, "*.txt")) + \
            glob.glob(os.path.join(KNOWLEDGE_FOLDER, "*.md"))
    
    if not files:
        return IngestResponse(status="skipped", files_processed=0, details="No files found.")

    documents = []
    for file_path in files:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                file_name = os.path.basename(file_path)
                logger.info(f"üìÑ Found file: {file_name} ({len(content)} characters)")
                logger.debug(f"   Content preview: {content[:100]}...")
                documents.append({
                    "document_id": file_name,
                    "content": content,
                    "mime_type": "text/plain",
                    "metadata": {"filename": file_name}
                })
        except Exception as e:
            logger.error(f"‚ùå Failed to read file {file_path}: {e}")
    
    logger.info(f"üì¶ Prepared {len(documents)} documents for ingestion")

    try:
        get_or_create_vector_db()
        
        # Strategy: Try SDK Tool -> SDK VectorIO -> REST
        ingested = False

        if not ingested:
            logger.info("Using REST API to ingest documents")
            raw_ingest_documents(documents)

        logger.info(f"‚úÖ Ingestion completed - {len(documents)} files processed")
        return IngestResponse(status="success", files_processed=len(documents), details=f"Inserted {len(documents)} files into vector database.")
    except Exception as e:
        logger.error(f"‚ùå Ingestion failed: {e}")
        traceback.logger.info_exc()
        raise HTTPException(status_code=500, detail=f"Ingestion failed: {str(e)}")

@app.get("/files-in-vector-db", response_model=List[str])
def files_in_vector_db():
    """
    Get the files in the vector database (knowledge folder).
    Returns only .txt and .md files.
    """
    if not os.path.exists(KNOWLEDGE_FOLDER):
        return []
    
    files_and_directories = os.listdir(KNOWLEDGE_FOLDER)
    # Filter for only .txt and .md files
    files = [
        f for f in files_and_directories 
        if os.path.isfile(os.path.join(KNOWLEDGE_FOLDER, f)) 
        and f.endswith(('.txt', '.md'))
    ]
    
    return sorted(files)